# -*- coding: utf-8 -*-
"""Amazon_analysis_Shan_Singh_Udemy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AxwDXNu1Y4qaRZo1IKURVdFCId31_lV1
"""

### Import necessary packages ..

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""The column or features in the dataset:
    
    Id
    ProductId — unique identifier for the product
    UserId — unqiue identifier for the user
    ProfileName
    HelpfulnessNumerator — number of users who found the review helpful
    HelpfulnessDenominator — number of users who indicated whether they found the review helpful or not
    Score — rating between 1 and 5
    Time — timestamp for the review
    Summary — brief summary of the review
    Text — text of the review

## 1.. reading data from Sqlite database
"""

import sqlite3

# Create a SQL connection to our SQLite database
con = sqlite3.connect(r'Z:\Data_Analysis_Projects\Amazon/database.sqlite')

type(con)

"""#### reading data from Sqlite database"""

df = pd.read_sql_query("SELECT * FROM REVIEWS" , con)

df.shape ## checking dimensions of df dataframe ..

"""## 2.. Data Preparation !
    Doing basic cleaning/Data wrangling(remove invalid rows , remove duplicate rows ..) & convert "time" feature data-type
    to date-time
"""

type(df) ## ie df is a dataframe

df.head(4)

df.columns

'''

HelpfulnessDenominator is : people found useful count + people found not so useful count (ie how many people reviewed )
Helpfulnessnumerator is among those how many found it helpful…..consider it like a percentage….

So, from this we can see that HelfulnessNumerator is always less than or equal to HelpfulnesDenominator..


'''

df['HelpfulnessNumerator'] > df['HelpfulnessDenominator']

df[df['HelpfulnessNumerator'] > df['HelpfulnessDenominator']]   ## invalid rows

df_valid = df[df['HelpfulnessNumerator'] <= df['HelpfulnessDenominator']]  ## valid rows

df_valid.shape

"""### lets find out whether instances/rows are duplicate or not !

Deduplication means removing duplicate rows, It is necessary to remove duplicates in order to get unbaised results,
    
    Checking duplicates based on UserId, ProfileName, Time, Text as No user can type a review on same exact time for
    different products, so we will remove those records
"""

df_valid.columns

df_valid.duplicated(['UserId', 'ProfileName' ,'Time' ,'Text'])

df_valid[df_valid.duplicated(['UserId', 'ProfileName' ,'Time' ,'Text'])]

### ie , approx 174521 are duplicated .. so u have to remove these duplicate rows ..

data = df_valid.drop_duplicates(subset=['UserId', 'ProfileName' ,'Time' ,'Text'])

data.shape

data.dtypes

"""#### Time — timestamp for the review
    convert time feature data-type from int64 to date-time !
"""

data['Time']

pd.to_datetime(data['Time'])

### as default is : unix start time "1970-01-01"

### 00:00:01.303862400 ->> by-default this time is in "nano-second" as default value of unit = "ns"
### so lets customize "unit" parameter to get time in second ..

data['Time'] = pd.to_datetime(data['Time'] , unit='s')

import warnings
from warnings import filterwarnings
filterwarnings('ignore')

"""## 3.. Analyse to what User Amazon Can recommend more product ?

#### Amazon can recommend more products to only those who are going to buy more or to one who has a better conversion rate,so lets ready data according to this problem statement
"""

data.shape

data.columns

data['ProfileName']

data['ProfileName'].nunique()

### Total unique profile-name are 218418
## Note : Profile_Name could be same but User-ID will be different , so lets consider UserID for this analysis ..

data['UserId'].nunique()

## ie total users are 256059

data.columns

data.groupby(['UserId']).agg({'Summary':'count' , 'Text':'count' , 'Score':'mean' ,'ProductId':'count'  })

### Since we want features as : 'Number_of_summaries','number_of_text','Avg_score','Number_of_products_purchased'
## Hence calling above aggregations makes sense :

recommend_df = data.groupby(['UserId']).agg({'Summary':'count' , 'Text':'count' , 'Score':'mean' ,'ProductId':'count'  }).sort_values(by='ProductId' , ascending=False)

recommend_df.columns = ['Number_of_summaries' , 'num_text' , 'avg_score' , 'No_of_prods_purchased']

recommend_df

recommend_df.index[0:10]

recommend_df['No_of_prods_purchased'][0:10].values

plt.bar(recommend_df.index[0:10] , recommend_df['No_of_prods_purchased'][0:10].values)
plt.xticks(rotation='vertical')

"""## 4.. which product has good number of reviews ?"""

data.columns

len(data['ProductId'].unique())

## We can see that there are 67624 types of products in the dataset.

prod_count = data['ProductId'].value_counts().to_frame()

prod_count

prod_count['ProductId']>500

prod_count[prod_count['ProductId']>500]

freq_prod_ids = prod_count[prod_count['ProductId']>500].index

freq_prod_ids  ## most sold products

data['ProductId'].isin(freq_prod_ids)

fre_prod_df = data[data['ProductId'].isin(freq_prod_ids)]

fre_prod_df

fre_prod_df.columns

sns.countplot(y = 'ProductId' , data = fre_prod_df , hue='Score')

"""## 5.. Is there any difference between behaviour of frequent viewers & not frequent viewers ?"""

data.columns

x = data['UserId'].value_counts()

x

data.head(7)

x

x['AY12DBB0U420B']

## "AY12DBB0U420B" has bought 329 products ..

data['viewer_type'] = data['UserId'].apply(lambda user : "Frequent" if x[user]>50 else "Not Frequent")

data.head(3)

data['viewer_type'].unique()

data['viewer_type']=='Not Frequent'

not_freq_df = data[data['viewer_type']=='Not Frequent']
freq_df = data[data['viewer_type']=='Frequent']

freq_df['Score'].value_counts()

freq_df['Score'].value_counts()/len(freq_df)*100

not_freq_df['Score'].value_counts()/len(not_freq_df)*100

### lets plot above results using bar-plot !
freq_df['Score'].value_counts().plot(kind='bar')

not_freq_df['Score'].value_counts().plot(kind='bar')

"""## 6.. Are frequent users more verbose ?  ( ie someone who speaks or post alot )"""

data.columns

data['Text']

data[['UserId' , 'ProductId' , 'Text']]

data['Text'][0]

type(data['Text'][0])

type(data['Text'][0].split(' '))

### we can call split on string data to convert into list based on Separator that I will pass so that I will compute total
## total words in feedback

len(data['Text'][0].split(' '))

def calculate_length(text):
    return len(text.split(' '))

data['Text_length'] = data['Text'].apply(calculate_length)

## lets separate dataframe for both "frequent_viewers" & for "not_frequent_viewers" ..

data['viewer_type'].unique()

not_freq_data = data[data['viewer_type']=='Not Frequent']
freq_data = data[data['viewer_type']=='Frequent']

not_freq_data

# define figure so that u can customize as per your need !
fig = plt.figure()

#add subplots
ax1 = fig.add_subplot(121)
ax1.boxplot(freq_data['Text_length'])
ax1.set_xlabel('Freq of freq reviewers') ## adding xlabel
ax1.set_ylim(0,600) ## setting limit on y-axis..

ax2 = fig.add_subplot(122)
ax2.boxplot(not_freq_data['Text_length'])
ax2.set_xlabel('Freq of not-freq reviewers')
ax2.set_ylim(0,600)

"""## 7.. sentiment analysis !

#### What is sentiment analysis?
    Sentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text
    Some examples of applications for sentiment analysis include:

    1. Analyzing the social media discussion around a certain topic
    2. Evaluating survey responses
    3. Determining whether product reviews are positive or negative
"""

#!pip install textblob

from textblob import TextBlob

data['Summary'][0]

TextBlob('Good Quality Dog Food').sentiment.polarity

data.shape

sample = data[0:50000]

### if u do not have good specifications , then its good to consider sample of data ..



polarity = []

for text in sample['Summary']: # list which will contain the polarity of the Summaries
    try:
        polarity.append(TextBlob(text).sentiment.polarity)
    except:
        polarity.append(0)

len(polarity)

sample['polarity'] = polarity

sample.head()

sample_negative= sample[sample['polarity']<0]

sample_positive= sample[sample['polarity']>0]

from collections import Counter

Counter(sample_negative['Summary']).most_common(10) ## most used negative keywords

Counter(sample_positive['Summary']).most_common(10) ## most used positive keywords